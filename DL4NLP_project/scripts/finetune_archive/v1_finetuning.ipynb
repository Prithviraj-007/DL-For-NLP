{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwWQ2Y9hkX2m",
        "outputId": "bea1941c-5873-4bde-a11d-05067d4054d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of dataset.zip or\n",
            "        dataset.zip.zip, and cannot find dataset.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!unzip \"dataset.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/liuhaotian/llava-v1.5-7b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-87MjaMk26m",
        "outputId": "417dd1d8-b743-45dc-aa96-f0004099b5ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'llava-v1.5-7b'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Total 19 (delta 0), reused 0 (delta 0), pack-reused 19\u001b[K\n",
            "Unpacking objects: 100% (19/19), 4.70 KiB | 801.00 KiB/s, done.\n",
            "Filtering content: 100% (4/4), 4.63 GiB | 7.91 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model-00001-of-00002.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "# Set the prompt and model versions directly in the command\n",
        "!deepspeed /LLaVA/llava/train/train_mem.py \\\n",
        "    --deepspeed /LLaVA/scripts/zero2.json \\\n",
        "    --lora_enable True \\\n",
        "    --lora_r 128 \\\n",
        "    --lora_alpha 256 \\\n",
        "    --mm_projector_lr 2e-5 \\\n",
        "    --bits 4 \\\n",
        "    --model_name_or_path /LLaVA/llava/llava-v1.5-7b \\\n",
        "    --version llava_llama_2 \\\n",
        "    --data_path /dataset/train/dataset.json \\\n",
        "    --validation_data_path /dataset/validation/dataset.json \\\n",
        "    --image_folder /dataset/images/ \\\n",
        "    --vision_tower openai/clip-vit-large-patch14-336 \\\n",
        "    --mm_projector_type mlp2x_gelu \\\n",
        "    --mm_vision_select_layer -2 \\\n",
        "    --mm_use_im_start_end False \\\n",
        "    --mm_use_im_patch_token False \\\n",
        "    --image_aspect_ratio pad \\\n",
        "    --group_by_modality_length True \\\n",
        "    --bf16 True \\\n",
        "    --output_dir /LLaVA/llava/checkpoints/llama-2-7b-chat-task-qlora \\\n",
        "    --num_train_epochs 500 \\\n",
        "    --per_device_train_batch_size 32 \\\n",
        "    --per_device_eval_batch_size 32 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --evaluation_strategy “epoch” \\\n",
        "    --save_strategy \"steps\" \\\n",
        "    --save_steps 50000 \\\n",
        "    --save_total_limit 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --weight_decay 0. \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --logging_steps 1 \\\n",
        "    --tf32 True \\\n",
        "    --model_max_length 2048 \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --dataloader_num_workers 4 \\\n",
        "    --lazy_preprocess True \\\n",
        "    --report_to wandb\n"
      ],
      "metadata": {
        "id": "Qsn-BDD1lQwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "\n",
        "!python run_llava.py --model-path /LLaVA/llava/checkpoints/llava-2-7b-chat-task-qlora/best_llava_eval_model_llava_lora\n",
        "--model-base /LLaVA/llava/llava-v1.5-7b\n",
        "--image-file /dataset/images/0f47c0b5-2c77-45e6-87b0-89af46e99500.jpg\n",
        "--query “why was this photo taken?”\n"
      ],
      "metadata": {
        "id": "BUSM80QAlQs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KlHiOkOplQpq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}